{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import dill\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import resource\n",
    "import sys\n",
    "resource.setrlimit(resource.RLIMIT_STACK, [0x100 * 0x100000, resource.RLIM_INFINITY])\n",
    "sys.setrecursionlimit(0x100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict():\n",
    "    return defaultdict(lambda: {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(URL):\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0' }\n",
    "    # fake user agent to attempt being detected as a web scraping bot\n",
    "    raw = requests.get(URL, headers=headers)\n",
    "    return bs(raw.content, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions(data_dict):\n",
    "    soup = get_soup('https://www.onthesnow.com/ski-resort.html')\n",
    "    regions = []\n",
    "    for x in soup.select('.country'):\n",
    "        for region in x.find('span'):\n",
    "            regions.append(str(region))       \n",
    "    \n",
    "    foo = []\n",
    "    for y in soup.select('.relatedRegions'):\n",
    "        sub_regions = []\n",
    "        for z in y.find_all('a'):\n",
    "            path = z['href']\n",
    "            name = z.text\n",
    "            sub_regions.append((str(name), str(path)))\n",
    "        foo.append(sub_regions)\n",
    "        \n",
    "    for place, sub_regions in enumerate(foo):\n",
    "        for sub_region in sub_regions:\n",
    "            data_dict[ regions[place] ][sub_region] = {}\n",
    "        \n",
    "    return dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resorts(data_dict):\n",
    "    for region in data_dict.keys():\n",
    "        for sub_region in data_dict[region].keys():\n",
    "            driver = webdriver.Chrome('/usr/bin/chromedriver')\n",
    "            driver.get('https://www.onthesnow.com/{}'.format(sub_region[1]))\n",
    "\n",
    "            for i in range(0, 40):\n",
    "                driver.execute_script('window.scrollBy(0, 600)')\n",
    "                time.sleep(0.4)\n",
    "\n",
    "            soup = bs(driver.page_source, features='lxml')\n",
    "            for x in soup.select('.name'):\n",
    "                for web_add in x.find_all(href=True):\n",
    "                    loc_name = web_add['title']\n",
    "                    loc_path = web_add['href']\n",
    "                    \n",
    "                    data_dict[region][sub_region][(str(loc_name), str(loc_path))] = {}\n",
    "\n",
    "            driver.quit()\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resort_data(data_dict):\n",
    "    years = np.arange(2008, 2018)\n",
    "    for region in data_dict.keys():\n",
    "        for sub_region in data_dict[region].keys():\n",
    "            print(sub_region)\n",
    "            for resort in data_dict[region][sub_region].keys():\n",
    "                for year in years:\n",
    "                    if year in data_dict[region][sub_region][resort].keys():\n",
    "                        # data was already populated previously - don't overwrite\n",
    "                        continue\n",
    "                    try:\n",
    "                        sf_vals, sf_dates = get_snowfall(resort, year)\n",
    "                        sd_vals, sd_dates = get_depth(resort, year)\n",
    "                    except:\n",
    "                        # chances are connection was closed due to web scraping bot being detected\n",
    "                        return data_dict\n",
    "                    if (sf_vals is None) or (sd_vals is None):\n",
    "                        # No data for that year\n",
    "                        continue\n",
    "                    elif sf_dates != sd_dates:\n",
    "                        raise ValueError('Snowfall dates do not match snow depth dates')\n",
    "                    else:\n",
    "                        data_dict[region][sub_region][resort][year] = {}\n",
    "                        data_dict[region][sub_region][resort][year]['snowfall'] = str_to_int(sf_vals)\n",
    "                        data_dict[region][sub_region][resort][year]['dates'] = date_to_datetime(sf_dates)\n",
    "                        data_dict[region][sub_region][resort][year]['depth'] = str_to_int(sd_vals)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_from_path(path):\n",
    "    return re.search('\\/[^\\/]*\\/[^\\/]*', path).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_datetime(lst):\n",
    "    out = []\n",
    "    for date in lst:\n",
    "        try:\n",
    "            out.append(datetime.strptime(date, '\"%Y-%m-%dT%H:%M:%S.%fZ\"').date())\n",
    "        except:\n",
    "            out.append(np.nan)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(lst):\n",
    "    out = []\n",
    "    for num in lst:\n",
    "        try:\n",
    "            out.append(int(num))\n",
    "        except:\n",
    "            out.append(0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowfall(resort, year):\n",
    "    name_path = name_from_path(resort[1])\n",
    "    soup = get_soup('https://www.onthesnow.com{}/historical-snowfall.html?&y={}&q=snow'.format(name_path, year))\n",
    "    for x in soup.find_all('div', attrs={'class': 'resBox'}):\n",
    "        val_regex = 'var jssnowfalls{} = \\[(.*?)\\];'.format(year)\n",
    "        date_regex = 'var jsdates{} = \\[(.*?)\\];'.format(year)\n",
    "        m1 = re.search(val_regex, x.get_text())\n",
    "        m2 = re.search(date_regex, x.get_text())\n",
    "        if m1 and m2:\n",
    "            vals = m1.group(1).split(',')\n",
    "            dates = m2.group(1).split(',')\n",
    "\n",
    "            if len(vals) != len(dates):\n",
    "                raise ValueError('Values do not match number of dates')\n",
    "            return vals, dates\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(resort, year):\n",
    "    name_path = name_from_path(resort[1])\n",
    "    soup = get_soup('https://www.onthesnow.com{}/historical-snowfall.html?&y={}&q=top'.format(name_path, year))\n",
    "    for x in soup.find_all('div', attrs={'class': 'resBox'}):\n",
    "        val_regex = 'var jssnowfalls{} = \\[(.*?)\\];'.format(year)\n",
    "        date_regex = 'var jsdates{} = \\[(.*?)\\];'.format(year)\n",
    "        m1 = re.search(val_regex, x.get_text())\n",
    "        m2 = re.search(date_regex, x.get_text())\n",
    "        if m1 and m2:\n",
    "            vals = m1.group(1).split(',')\n",
    "            dates = m2.group(1).split(',')\n",
    "\n",
    "            if len(vals) != len(dates):\n",
    "                raise ValueError('Values do not match number of dates')\n",
    "            return vals, dates\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_dict(output_file, dic):\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(dic, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dict(input_file):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dill_dict(output_file, dic):\n",
    "    with open(output_file, 'wb') as f:\n",
    "        dill.dump(dic, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    try:\n",
    "        data_dict = open_dict('data/snow.pkl')\n",
    "    except:\n",
    "        print('Unable to open full data dictionary')\n",
    "        \n",
    "        try:\n",
    "            data_dict = open_dict('data/resorts.pkl')\n",
    "        except:\n",
    "            print('Unable to open partial data dictionary (resort info)')\n",
    "            data_dict = create_data_dict()\n",
    "            data_dict = get_regions(data_dict)\n",
    "            data_dict = get_resorts(data_dict)\n",
    "            \n",
    "            try:\n",
    "                pickle_dict('data/resorts.pkl', data_dict)\n",
    "            except:\n",
    "                print('Unable to pickle partial data dictionary (resort info)')\n",
    "    \n",
    "    data_dict = resort_data(data_dict)\n",
    "    \n",
    "    try:\n",
    "#         dill.detect.trace(True)\n",
    "#         dill.detect.errors(data_dict)\n",
    "        pickle_dict('data/snow.pkl', data_dict)\n",
    "    except:\n",
    "        print('Unable to pickle full data dictionary')\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Alaska', '/alaska/ski-resorts.html')\n",
      "('Arizona', '/arizona/ski-resorts.html')\n",
      "('California', '/california/ski-resorts.html')\n"
     ]
    }
   ],
   "source": [
    "foo = run()\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in foo.keys():\n",
    "    for sub_region in foo[region].keys():\n",
    "        if not isinstance(sub_region, tuple):\n",
    "            print(region,'-',sub_region)\n",
    "        for resort in foo[region][sub_region].keys():\n",
    "            if not isinstance(resort, tuple):\n",
    "                print(region,'-',sub_region,'-',resort)\n",
    "            elif not isinstance(foo[region][sub_region][resort], dict):\n",
    "                print(region,'-',sub_region,'-',resort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
