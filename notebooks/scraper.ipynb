{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import dill\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import resource\n",
    "import sys\n",
    "import logging\n",
    "resource.setrlimit(resource.RLIMIT_STACK, [0x100 * 0x100000, resource.RLIM_INFINITY])\n",
    "sys.setrecursionlimit(0x100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_dict():\n",
    "    return defaultdict(lambda: {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(URL, inc=0):\n",
    "    headers = {'User-Agent': 'Mozilla/{}'.format(float(4 + inc))} #'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0'\n",
    "    # fake user agent to attempt being detected as a web scraping bot\n",
    "    raw = requests.get(URL, headers=headers)\n",
    "    return bs(raw.content, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regions(data_dict):\n",
    "    soup = get_soup('https://www.onthesnow.com/ski-resort.html')\n",
    "    regions = []\n",
    "    for x in soup.select('.country'):\n",
    "        for region in x.find('span'):\n",
    "            regions.append(str(region))       \n",
    "    \n",
    "    foo = []\n",
    "    for y in soup.select('.relatedRegions'):\n",
    "        sub_regions = []\n",
    "        for z in y.find_all('a'):\n",
    "            path = z['href']\n",
    "            name = z.text\n",
    "            sub_regions.append((str(name), str(path)))\n",
    "        foo.append(sub_regions)\n",
    "        \n",
    "    for place, sub_regions in enumerate(foo):\n",
    "        for sub_region in sub_regions:\n",
    "            data_dict[ regions[place] ][sub_region] = {}\n",
    "        \n",
    "    return dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resorts(data_dict):\n",
    "    for region in data_dict.keys():\n",
    "        for sub_region in data_dict[region].keys():\n",
    "            driver = webdriver.Chrome('/usr/bin/chromedriver')\n",
    "            driver.get('https://www.onthesnow.com/{}'.format(sub_region[1]))\n",
    "\n",
    "            for i in range(0, 40):\n",
    "                driver.execute_script('window.scrollBy(0, 600)')\n",
    "                time.sleep(0.4)\n",
    "\n",
    "            soup = bs(driver.page_source, features='lxml')\n",
    "            for x in soup.select('.name'):\n",
    "                for web_add in x.find_all(href=True):\n",
    "                    loc_name = web_add['title']\n",
    "                    loc_path = web_add['href']\n",
    "                    \n",
    "                    data_dict[region][sub_region][(str(loc_name), str(loc_path))] = {}\n",
    "\n",
    "            driver.quit()\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipped_added(skipped, added):\n",
    "    if skipped and added:\n",
    "        print('\\t*some data was added and some data was skipped...')\n",
    "    elif skipped:\n",
    "        print('\\t*all years were skipped due to being previously filled in...')\n",
    "    elif added:\n",
    "        print('\\t*new data was added for all years')\n",
    "    else:\n",
    "        print('\\t***SOMETHING WENT WRONG***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resort_data(data_dict, finished_regions):\n",
    "    inc = 0\n",
    "    years = np.arange(2008, 2018)\n",
    "    for region in data_dict.keys():\n",
    "        if finished_regions[region]:\n",
    "            # skip over region if all data has been obtained\n",
    "            print('Skipping {}...'.format(region))\n",
    "            continue\n",
    "        sub_regions = data_dict[region]\n",
    "        for sub_region in sub_regions.keys():\n",
    "            skipped = False\n",
    "            added = False\n",
    "            print(sub_region[0])\n",
    "            \n",
    "            resorts = sub_regions[sub_region]\n",
    "            for resort in resorts.keys():\n",
    "                pre_filled_years = resorts[resort].keys()\n",
    "                for year in years:\n",
    "                    if year in pre_filled_years:\n",
    "                        # data was already populated previously - don't overwrite\n",
    "                        skipped = True\n",
    "                        continue\n",
    "                        \n",
    "                    empty = True # data has not been retrieved by request\n",
    "                    count = 0\n",
    "                    while empty:\n",
    "                        try:\n",
    "                            sf_vals, sf_dates = get_snowfall(resort, year, inc)\n",
    "                            sd_vals, sd_dates = get_depth(resort, year, inc)\n",
    "                            empty = False\n",
    "                        except:\n",
    "                            # chances are connection was closed due to web scraping bot being detected\n",
    "                            logging.exception('Failure within get_snowfall or get_depth')\n",
    "                            count += 1 # keep track of failed attempts\n",
    "                            inc += 0.1 # change (fake) browser version\n",
    "                            if count >= 10:\n",
    "                                # getting soup failed 10 times in a row, so exit\n",
    "                                return data_dict\n",
    "                            \n",
    "                    if (sf_vals is None) or (sd_vals is None):\n",
    "                        # No data for that year\n",
    "                        continue\n",
    "                    elif sf_dates != sd_dates:\n",
    "                        raise ValueError('Snowfall dates do not match snow depth dates')\n",
    "                    else:\n",
    "                        data_dict[region][sub_region][resort][year] = {}\n",
    "                        data_dict[region][sub_region][resort][year]['snowfall'] = str_to_int(sf_vals)\n",
    "                        data_dict[region][sub_region][resort][year]['dates'] = date_to_datetime(sf_dates)\n",
    "                        data_dict[region][sub_region][resort][year]['depth'] = str_to_int(sd_vals)\n",
    "                        added = True\n",
    "            skipped_added(skipped, added)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_from_path(path):\n",
    "    return re.search('\\/[^\\/]*\\/[^\\/]*', path).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_datetime(lst):\n",
    "    out = []\n",
    "    for date in lst:\n",
    "        try:\n",
    "            out.append(datetime.strptime(date, '\"%Y-%m-%dT%H:%M:%S.%fZ\"').date())\n",
    "        except:\n",
    "            out.append(np.nan)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(lst):\n",
    "    out = []\n",
    "    for num in lst:\n",
    "        try:\n",
    "            out.append(int(num))\n",
    "        except:\n",
    "            out.append(0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snowfall(resort, year, inc):\n",
    "    name_path = name_from_path(resort[1])\n",
    "    soup = get_soup('https://www.onthesnow.com{}/historical-snowfall.html?&y={}&q=snow'.format(name_path, year), inc)\n",
    "    for x in soup.find_all('div', attrs={'class': 'resBox'}):\n",
    "        val_regex = 'var jssnowfalls{} = \\[(.*?)\\];'.format(year)\n",
    "        date_regex = 'var jsdates{} = \\[(.*?)\\];'.format(year)\n",
    "        m1 = re.search(val_regex, x.get_text())\n",
    "        m2 = re.search(date_regex, x.get_text())\n",
    "        if m1 and m2:\n",
    "            vals = m1.group(1).split(',')\n",
    "            dates = m2.group(1).split(',')\n",
    "\n",
    "            if len(vals) != len(dates):\n",
    "                raise ValueError('Values do not match number of dates')\n",
    "            return vals, dates\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(resort, year, inc):\n",
    "    name_path = name_from_path(resort[1])\n",
    "    soup = get_soup('https://www.onthesnow.com{}/historical-snowfall.html?&y={}&q=top'.format(name_path, year), inc)\n",
    "    for x in soup.find_all('div', attrs={'class': 'resBox'}):\n",
    "        val_regex = 'var jssnowfalls{} = \\[(.*?)\\];'.format(year)\n",
    "        date_regex = 'var jsdates{} = \\[(.*?)\\];'.format(year)\n",
    "        m1 = re.search(val_regex, x.get_text())\n",
    "        m2 = re.search(date_regex, x.get_text())\n",
    "        if m1 and m2:\n",
    "            vals = m1.group(1).split(',')\n",
    "            dates = m2.group(1).split(',')\n",
    "\n",
    "            if len(vals) != len(dates):\n",
    "                raise ValueError('Values do not match number of dates')\n",
    "            return vals, dates\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_dict(output_file, dic):\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(dic, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dict(input_file):\n",
    "    with open(input_file, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dill_dict(output_file, dic):\n",
    "    with open(output_file, 'wb') as f:\n",
    "        dill.dump(dic, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(resort_file, snow_file, finished_regions):\n",
    "    try:\n",
    "        data_dict = open_dict(snow_file)\n",
    "    except:\n",
    "        print('Unable to open full data dictionary')\n",
    "        \n",
    "        try:\n",
    "            data_dict = open_dict(resort_file)\n",
    "        except:\n",
    "            print('Unable to open partial data dictionary (resort info)')\n",
    "            data_dict = create_data_dict()\n",
    "            data_dict = get_regions(data_dict)\n",
    "            data_dict = get_resorts(data_dict)\n",
    "            \n",
    "            try:\n",
    "                pickle_dict(resort_file, data_dict)\n",
    "            except:\n",
    "                print('Unable to pickle partial data dictionary (resort info)')\n",
    "    \n",
    "    data_dict = resort_data(data_dict, finished_regions)\n",
    "    \n",
    "    try:\n",
    "#         dill.detect.trace(True)\n",
    "#         dill.detect.errors(data_dict)\n",
    "        pickle_dict(snow_file, data_dict)\n",
    "    except:\n",
    "        print('Unable to pickle full data dictionary')\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alaska\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Arizona\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "California\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Colorado\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Connecticut\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Idaho\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Illinois\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Indiana\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Iowa\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Maine\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Maryland\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Massachusetts\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Michigan\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Minnesota\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Missouri\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Montana\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Nevada\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "New Hampshire\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "New Jersey\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "New Mexico\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "New York\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "North Carolina\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Ohio\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Oregon\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Pennsylvania\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "South Dakota\n",
      "\t*all years were skipped due to being previously filled in...\n",
      "Tennessee\n"
     ]
    }
   ],
   "source": [
    "resort_file = '../data/resorts.pkl'\n",
    "snow_file = '../data/snow.pkl'\n",
    "finished_regions = {'United States': False, 'Canada': False, 'Europe': False, 'South America': False}\n",
    "\n",
    "foo = run(resort_file, snow_file, finished_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
